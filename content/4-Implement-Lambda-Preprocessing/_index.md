+++
title = "Implementing a Lambda Preprocessing Function"
date = 2025
weight = 4
chapter = false
pre = "<b>4. </b>"
+++

In this section, we will build a **Lambda Preprocessing Function** to automatically process raw data uploaded to S3. This is an important **initial** step in the machine learning pipeline â€” ensuring the data is clean, standardized, and ready for training models on Amazon SageMaker.

---

### ğŸ¯ Goals

- Build a Lambda function in **Python** to process CSV data from S3.

- Automatically trigger Lambda when a new file is added to the `raw/` directory.

- Write the processed results to the `processed/` directory in the same bucket.

- Ensure the data is ready for training a SageMaker model.

---

### ğŸ§  Architecture Overview

Here's how Lambda Preprocessing Function works in the entire pipeline:

~~~
ğŸ“ S3 Bucket
â”œâ”€â”€ raw/ â† raw CSV uploaded
â”œâ”€â”€ processed/ â† CSV processed, ready to train
~~~

- **Step 1:** User or system uploads CSV file to `raw/`.

- **Step 2:** S3 activates Lambda via trigger.

- **Step 3:** Lambda reads file, cleans data (removes error lines, handles empty valuesâ€¦).

- **Step 4:** Lambda writes processed file to `processed/`.

---

#### ğŸ› ï¸ 4.1 â€“ Create Lambda Function

1. Go to [AWS Lambda Console](https://console.aws.amazon.com/lambda/home).

2. Click **Create function**.

3. Configuration:

- **Function name:** `preprocessData`
- **Runtime:** Python 3.9
- **Role:** Select the IAM role created in the previous step (with S3 permissions).

![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.1.png)
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.2.png)
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.3.png)

---

#### ğŸ“œ 4.2 â€“ Write code to process data

In the **Code** tab, replace the default content with the following code:

```python
import boto3
import csv
import io
import os

s3 = boto3.client('s3')

def lambda_handler(event, context):
    # Láº¥y bucket vÃ  key cá»§a file má»›i upload
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']

    print(f"Processing file: s3://{bucket}/{key}")

    # Äá»c file CSV tá»« S3
    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        raw_bytes = response['Body'].read()
        try:
            content = raw_bytes.decode('utf-8')       # Thá»­ decode UTF-8
        except UnicodeDecodeError:
            content = raw_bytes.decode('utf-16')      # Náº¿u lá»—i, thá»­ UTF-16
    except Exception as e:
        print(f"Error reading file from S3: {e}")
        raise e

    reader = csv.reader(io.StringIO(content))

    processed_rows = []
    for row in reader:
        # Bá» qua dÃ²ng rá»—ng hoáº·c lá»—i
        if all(row):
            processed_rows.append(row)

    # Viáº¿t file Ä‘Ã£ xá»­ lÃ½ vÃ o folder processed/
    output_prefix = os.getenv('OUTPUT_PREFIX', 'processed/')
    output_key = os.path.join(output_prefix, os.path.basename(key))

    output_content = io.StringIO()
    writer = csv.writer(output_content)
    writer.writerows(processed_rows)

    # Upload file Ä‘Ã£ xá»­ lÃ½ lÃªn S3
    try:
        s3.put_object(
            Bucket=bucket,
            Key=output_key,
            Body=output_content.getvalue().encode('utf-8'),
            ContentType='text/csv'
        )
        print(f"Processed file saved to s3://{bucket}/{output_key}")
    except Exception as e:
        print(f"Error writing file to S3: {e}")
        raise e

    return {
        'statusCode': 200,
        'body': f"Processed file saved to {output_key}"
    }
```
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.4.png)

#### âš™ï¸ 4.3 â€“ Assign Environment Variables

In the Configuration â†’ Environment variables tab, click Edit and add:

- Key: OUTPUT_PREFIX
- Value: processed/

{{% notice tip %}}
Environment variables help you easily change the output path without editing the source code.
{{% /notice %}}
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.5.png)
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.6.png)
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.7.png)
#### ğŸ“¦ 4.4 â€“ Package and Upload Lambda (local option)

If developing on a local computer:
~~~~
pip install -r requirements.txt -t .
zip -r preprocessData.zip .
~~~~

Then go back to Lambda Console â†’ Code tab â†’ Upload from â†’ .zip file.

Create a requirements.txt file with the following content:

~~~
boto3
~~~

#### ğŸ”” 4.5 â€“ Connect Lambda to S3 (Trigger)

In Lambda Console, go to Configuration â†’ Triggers tab.

Click Add trigger.

Select:
~~~~
Trigger type: S3

Bucket: your-raw-data-bucket

Event type: PUT

Prefix: raw/
~~~~
ğŸ“¸ Trigger configuration example:

{{% notice warning %}}
Make sure the bucket and Lambda are in the same region. Otherwise, the trigger will not work.
{{% /notice %}}
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.8.png)
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.9.png)
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.10.png)


#### ğŸ§ª 4.6 â€“ Test the Preprocessing Function

Upload a sample CSV file to the raw/ folder:
~~~
aws s3 cp data.csv s3://ml-pipeline-bucket/raw/data.csv
~~~

Lambda will automatically run and create the processed file in processed/.
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.12.png)
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.14.png)
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.15.png)
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.11.png)
ğŸ“Š Example result:
~~~
Input: s3://ml-pipeline-bucket/raw/data.csv

Output: s3://ml-pipeline-bucket/processed/data.csv
~~~
![create-lambda.png](/images/4-Implement-Lambda-Preprocessing/4.13.png)

#### âœ… Done

ğŸ‰ You have successfully deployed a Lambda Preprocessing Function â€“ the first step in a machine learning pipeline.