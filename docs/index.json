[
{
	"uri": "//localhost:1313/1-introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Workshop: Building an End-to-End Machine Learning Pipeline on AWS with Lambda, API Gateway, S3, SageMaker \u0026amp; DynamoDB In this workshop, you will learn how to build and deploy a complete end-to-end machine learning pipeline on AWS.\nWe will use AWS Lambda for preprocessing and inference, Amazon API Gateway to expose RESTful APIs, Amazon S3 for data storage, Amazon SageMaker for model training and deployment, Amazon DynamoDB for metadata storage, and Amazon CloudWatch for monitoring and logging.\nObjectives Understand how to design and deploy an ML pipeline on AWS from raw data ingestion to model deployment. Implement data preprocessing and model inference with AWS Lambda. Train and register models using Amazon SageMaker. Expose RESTful endpoints for inference using Amazon API Gateway. Store metadata and inference logs in Amazon DynamoDB. Monitor system performance with Amazon CloudWatch. Learn cost management and AWS Free Tier optimization strategies. Requirements An AWS account (Free Tier: https://aws.amazon.com/free) Basic Python skills (for Lambda functions and ML scripts) Familiarity with REST APIs and JSON Tools: AWS CLI, Git, Docker (optional), web browser (Optional) Postman for testing APIs Architecture Overview The solution consists of several serverless and managed AWS services working together to automate the ML workflow:\n1. AWS Lambda\nUses:\nData preprocessing before training (e.g., cleaning, feature extraction). Running inference on new incoming data. Integrating different stages of the pipeline. Cost:\nStatus Requests Cost/day Cost/month Free Tier 1M requests $0.00 $0.00 After Free Tier 100K requests ~$0.01 ~$0.30 2. Amazon API Gateway\nUses:\nExposes RESTful endpoints for triggering Lambda functions and model inference. Acts as the interface between clients and backend services. Cost:\nStatus Requests Cost/day Cost/month Free Tier 1M requests $0.00 $0.00 After Free Tier 3M requests ~$0.35 ~$10.50 3. Amazon S3\nUses:\nStores raw datasets, preprocessing outputs, and model artifacts. Centralized data lake for the ML workflow. Cost:\nStatus Storage Cost/day Cost/month Free Tier 5GB storage $0.00 $0.00 After Free Tier 5GB ~$0.03 ~$1.00 4. Amazon SageMaker\nUses:\nTrains and registers machine learning models. Hosts models as managed endpoints for real-time inference. Cost:\nStatus Type Cost/day Cost/month Trial 250 hours/month $0.00 $0.00 After Free Tier ml.t2.medium ~$0.10 ~$3.00 5. Amazon DynamoDB\nUses:\nStores metadata such as model version, training metrics, and inference logs. Provides low-latency access for serverless apps. Cost:\nStatus Capacity Cost/day Cost/month Free Tier 25 read/write units $0.00 $0.00 After Free Tier 10K read/write ~$0.02 ~$0.60 6. Amazon CloudWatch\nUses:\nMonitors logs, tracks Lambda execution metrics, and provides alerts. Cost:\nTypically Free for basic metrics. Cost Summary Service Cost (Free Tier) Cost (After Free Tier) AWS Lambda $0.00 ~$0.30 Amazon API Gateway $0.00 ~$10.50 Amazon S3 $0.00 ~$1.00 Amazon SageMaker $0.00 ~$3.00 Amazon DynamoDB $0.00 ~$0.60 Amazon CloudWatch $0.00 ~$0.00 Total $0.00 ~$15.40/month Workflow Data Ingestion â€“ Raw data is uploaded to S3. Preprocessing â€“ A Lambda function cleans and transforms the data. Training â€“ SageMaker trains the model and registers it in the model registry. Deployment â€“ The model is deployed as a SageMaker endpoint. Inference â€“ Lambda + API Gateway provide real-time predictions to client applications. Metadata \u0026amp; Monitoring â€“ DynamoDB stores metadata, CloudWatch monitors logs and metrics. Conclusion This workshop shows how to build a production-ready, scalable ML pipeline entirely on AWS using serverless and managed services.\nIt minimizes operational overhead, scales automatically, and remains cost-effective â€” often $0 during the Free Tier and around $15/month after that, depending on traffic and usage.\n"
},
{
	"uri": "//localhost:1313/5-train-model-with-sagemaker/5.1-prepare-training-data/",
	"title": "Preparing Input Data for SageMaker",
	"tags": [],
	"description": "",
	"content": "Before we start training a machine learning model with Amazon SageMaker, we need to check and organize the pre-processed data from Lambda. This is an important step to ensure the training process is accurate and efficient.\nğŸ§  The role of data in the pipeline Data is the â€œfuelâ€ of the ML model. The quality and structure of the input data will directly affect:\nğŸ“Š Training performance â€“ the model learns better when the data has been cleaned. ğŸ” Prediction accuracy â€“ clean and properly formatted data helps the model predict more accurately. âš™ï¸ Pipeline Automation â€“ SageMaker requires a consistent data structure to create training jobs. ğŸª„ 1. Check the processed data on S3 Go to Amazon S3 in the AWS Management Console to verify the input data from Lambda:\nSelect the bucket you created in section 3 â€“ Create S3 Bucket for Data Storage.\nOpen the processed/ folder â€“ this is where Lambda saved the pre-processed data.\nCheck if the CSV or Parquet file exists (e.g. data_processed.csv).\nğŸ“¸ Example folder structure:\nml-pipeline-bucket/ â”œâ”€ raw/ â”‚ â””â”€ data.csv â””â”€ processed/ â””â”€ data_processed.csv ğŸ’¡ The data in the processed/ folder is the input that SageMaker uses in the next model training step.\nğŸ§± 2. Organize data properly in SageMaker SageMaker expects the input data to be in a specific folder in S3, for example:\ns3://ml-pipeline-bucket/processed/train/ s3://ml-pipeline-bucket/processed/validation/ You can organize the data as follows:\ntrain/ â€“ contains data used to train the model (~80%) validation/ â€“ contains data used to evaluate the model (~20%) ğŸ“Œ For example:\nprocessed/ â”œâ”€ train/ â”‚ â””â”€ train.csv â””â”€ validation/ â””â”€ val.csv ğŸ› ï¸ 3. Update S3 Access Permissions for SageMaker Ensure the SageMaker IAM Role has access to the bucket containing the data:\ns3:GetObject s3:ListBucket s3:PutObject (if write results are needed) âš ï¸ If SageMaker does not have permission to read data from the S3 bucket, the training job will fail immediately.\nğŸ” 4. Check the data format SageMaker supports CSV, Parquet, or RecordIO formats.\nIf using CSV, ensure:\nThere are headers describing the columns.\nThere are no null values â€‹â€‹or formatting errors.\nNumeric features have been normalized (if needed).\nExample of a standard train.csv file:\nfeature1,feature2,feature3,label 0.21,0.75,0.11,1 0.56,0.22,0.65,0 0.34,0.12,0.88,1 âœ… Done You have completed preparing the input data for SageMaker:\nğŸ“ The data has been preprocessed and saved to the processed/ folder on S3 ğŸ—‚ï¸ The data has been split into train/ and validation/ ğŸ” The IAM Role has the necessary access ğŸ§¹ The data format has been verified "
},
{
	"uri": "//localhost:1313/",
	"title": "Workshop: Building an End-to-End Machine Learning Pipeline on AWS with Lambda, API Gateway, S3, SageMaker &amp; DynamoDB",
	"tags": [],
	"description": "",
	"content": "Workshop: Building an End-to-End Machine Learning Pipeline on AWS with Lambda, API Gateway, S3, SageMaker \u0026amp; DynamoDB Overview In this workshop, you will learn how to build and deploy an end-to-end machine learning pipeline on AWS.\nWe will use AWS Lambda for preprocessing and inference, API Gateway to expose RESTful endpoints, S3 for data storage, Amazon SageMaker for model training and hosting, DynamoDB for metadata storage, and CloudWatch for monitoring and logging.\nğŸ¯ Objectives Understand how to design and deploy a complete ML pipeline on AWS. Build and configure data ingestion, preprocessing, and model training workflows. Deploy and manage machine learning models with Amazon SageMaker. Expose inference endpoints through API Gateway and Lambda. Integrate DynamoDB for model metadata and use CloudWatch for monitoring. Learn best practices for permissions, security, and cost optimization. ğŸ§° Requirements An existing AWS account (Free Tier: https://aws.amazon.com/free) Basic knowledge of Python or Go (for Lambda functions and ML scripts) Familiarity with REST APIs and JSON Tools: AWS CLI, Git, Docker (optional), and a web browser (Optional) Postman for testing inference endpoints ğŸ’¡ If you already have an AWS account with full access, you can skip IAM setup and continue directly to building resources.\nğŸ“š Contents Introduction Check-AWS-Account-and-Permissions Create-S3-Bucket-for-Data-Storage Implement-Lambda-Preprocessing-Function Train-and-Register-Model-with-Amazon-SageMaker Deploy-SageMaker-Endpoint-for-Inference Build-Lambda-Inference-Function-and-API-Gateway Integrate-DynamoDB-and-CloudWatch Clean-Up-Resources Conclusion-and-Key-Takeaways "
},
{
	"uri": "//localhost:1313/2-check-aws-account-and-permissions/",
	"title": "Check AWS Account and Permissions",
	"tags": [],
	"description": "",
	"content": " ğŸ’¡ Best practice: Do not use Root User for daily operations. Instead, create IAM Role and IAM Policy with minimum required permissions for Lambda, SageMaker and other services to access S3, DynamoDB, CloudWatch in your pipeline.\nIn this section, you will:\nCheck AWS account and region. Create IAM Policy to allow Lambda and SageMaker to access DynamoDB, S3, CloudWatch. Create IAM Role and assign it to Lambda. (Optional) create IAM Role for SageMaker. Test permissions using Lambda test. 1. Check AWS Account and Region Log in to AWS Management Console\nCheck Region in the upper right corner (use us-east-1 or ap-southeast-1 to align the pipeline).\nIf IAM is not enabled, go to IAM Console to enable the service.\nğŸ“ Note: Most AWS services operate by region. If you create a resource in one region and call it in another, the pipeline may not work or give a ResourceNotFound error.\n2. Create Custom IAM Policy We will create a policy to grant access to DynamoDB, S3, SageMaker, and CloudWatch to Lambda.\nNavigate to: IAM â†’ Policies â†’ Create policy Select the JSON tab and paste the following content: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;,\u0026#34;s3:PutObject\u0026#34;,\u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::your-bucket-name\u0026#34;, \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;dynamodb:PutItem\u0026#34;,\u0026#34;dynamodb:UpdateItem\u0026#34;,\u0026#34;dynamodb:GetItem\u0026#34;,\u0026#34;dynamodb:Scan\u0026#34;,\u0026#34;dynamodb:Query\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:us-east-1:*:table/ModelMetadata\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sagemaker:CreateTrainingJob\u0026#34;, \u0026#34;sagemaker:DescribeTrainingJob\u0026#34;, \u0026#34;sagemaker:CreateModel\u0026#34;, \u0026#34;sagemaker:CreateEndpoint\u0026#34;, \u0026#34;sagemaker:InvokeEndpoint\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;logs:CreateLogGroup\u0026#34;,\u0026#34;logs:CreateLogStream\u0026#34;,\u0026#34;logs:PutLogEvents\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } ğŸ” Explanation: DynamoDB permissions only allow Lambda to operate on the ModelMetadata table. S3 permissions are limited to the bucket you specify. logs:* is required for Lambda to log to CloudWatch.\nâš ï¸ Warning: Do not leave \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;*\u0026rdquo; for DynamoDB or S3 in production. Always limit to a specific ARN for added security.\n3. Create an IAM Role for Lambda Navigate to: IAM â†’ Roles â†’ Create role Select AWS Service â†’ Lambda â†’ Next Assign the newly created ml-pipeline-access-policy policy. Assign AWSLambdaBasicExecutionRole to enable logging. Name: lambda-ml-pipeline-role â†’ Create role ğŸ“˜ Information: This role will be assigned to all Lambda functions in the pipeline, for example: PreprocessLambda, InferenceLambda\u0026hellip;\n4. Assign IAM Role to Lambda Functions Go to Lambda Console â†’ Functions â†’ [Function name] â†’ Configuration â†’ Permissions Select Edit and change the role to lambda-ml-pipeline-role ğŸ“ Note: Assigning the wrong role is the most common cause of AccessDenied errors when Lambda accesses DynamoDB or S3.\n5. (Optional) Create IAM Role for SageMaker If you use SageMaker to train and deploy the model: Go to IAM â†’ Roles â†’ Create role Select SageMaker Assign ml-pipeline-access-policy Name: sagemaker-ml-pipeline-role 6. Check permissions and Test Lambda Go to IAM â†’ Roles â†’ lambda-ml-pipeline-role Make sure the policy is fully assigned and ARN the correct resource. Create a Test event in Lambda and run it: { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;Preprocessing complete and metadata updated\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; } } âš ï¸ If you get an AccessDenied error: Check the log in CloudWatch. Verify the region (us-east-1). Make sure the DynamoDB table name and S3 bucket match the ARN in the policy.\n"
},
{
	"uri": "//localhost:1313/5-train-model-with-sagemaker/5.2-create-training-job/",
	"title": "Create SageMaker Training Job",
	"tags": [],
	"description": "",
	"content": "In this section, we will initialize and configure a SageMaker Training Job to train a machine learning model using preprocessed data from Lambda and stored on S3.\nğŸ¯ Objectives Create Training Job on SageMaker using data in S3.\nConfigure training parameters such as container, instance type, S3 input/output.\nMonitor training progress and verify results.\nâš™ï¸ 1. Access Amazon SageMaker Go to AWS Management Console â†’ find and open Amazon SageMaker.\nIn the left navigation bar, select Training jobs â†’ Create training job.\nğŸ“ 2. Name and basic configuration Training job name: ml-pipeline-training-job\nIAM Role: Select a role that has access to S3 and SageMaker (e.g., SageMakerExecutionRole).\nAlgorithm source:\nSelect Your own algorithm container if you have a custom script.\nOr select Built-in algorithm (e.g., XGBoost) for quick testing.\nğŸ’¡ If this is your first time experimenting, we recommend choosing the XGBoost built-in container to simplify the training process.\nğŸ“¦ 3. Configure training data In the Input data configuration section:\nChannel name: train Input mode: File S3 location: s3://ml-pipeline-bucket/processed/train/ Add a new channel for validation:\nChannel name: validation S3 location: s3://ml-pipeline-bucket/processed/validation/ ğŸ“ The S3 structure should be as follows:\nml-pipeline-bucket/ â””â”€ processed/ â”œâ”€ train/ â”‚ â””â”€ train.csv â””â”€ validation/ â””â”€ val.csv âš™ï¸ 4. Configure training resources Instance type: ml.m5.large (or choose GPU instance if model requires) Instance count: 1 Volume size: 10 GB Max runtime: 3600 (limited to 1 hour of training) âš ï¸ Choose an instance size that fits your budget. Instances like ml.m5.large are in the Free Tier and are powerful enough for demos.\nğŸ“¤ 5. Set the location to save the model after training In the Output data configuration section:\nS3 output path: s3://ml-pipeline-bucket/model/ SageMaker will save the trained model file (e.g. model.tar.gz) here. ğŸ§ª 6. Initialize Training Job Check all the configurations again.\nClick Create training job to start the training process.\nğŸ“Š Interface when the job is running:\nğŸ” 7. Monitor progress and check results In the list of Training jobs, select the job you just created.\nCheck the status: InProgress â†’ Completed.\nView detailed logs in CloudWatch Logs to monitor the training process.\nOnce completed, the model file will be saved at: s3://ml-pipeline-bucket/model/model.tar.gz\nIf the job fails, check your IAM permissions and S3 path. Make sure the data in train/ and validation/ has a valid structure and format. âœ… Done You have successfully created a SageMaker Training Job and trained the model using preprocessed data from Lambda.\n"
},
{
	"uri": "//localhost:1313/3-create-s3-bucket/3.1-create-an-bucket-to-store-data/",
	"title": "Create an S3 Bucket to store data",
	"tags": [],
	"description": "",
	"content": "What is Amazon S3? Amazon S3 (Simple Storage Service) is an object storage service from AWS that is virtually infinitely scalable, highly durable, and easily integrated with other services such as Lambda, CloudFront, and API Gateway.\nIn this project, S3 will serve as a place to store all static data such as:\nğŸ—‚ï¸ Web Frontend built with React.js ğŸ–¼ï¸ Images and article content (media) ğŸ“¦ Temporary files or data needed for Lambda to process Why create an S3 Bucket? ğŸ“ Static website hosting: Upload the entire frontend (HTML, CSS, JS) after building so that the website can be accessed via CloudFront.\nğŸ” Integrated with CloudFront: S3 is the original data source for CDN to help optimize page loading speed.\nğŸ” Integrated with Lambda: Allow Lambda to access/write data when processing is needed.\nâš™ï¸ Serverless architecture: No need for a server to store content, everything runs completely serverless.\nğŸª£ 3.1. Create an S3 Bucket Access AWS Management Console\nOpen the Amazon S3 service.\nIn the left menu, select Buckets, then click Create bucket.\nIn the Create bucket interface: Bucket name: set a globally unique name, for example: my-serverless-blog-bucket AWS Region: select the same region as Lambda and DynamoDB (for example: us-east-1) Block Public Access: uncheck if you want the bucket to be public for frontend storage (will be adjusted later). Keep the other parts default. Click Create bucket ğŸ’¡ Hint: The bucket name must be globally unique â€“ if the name you enter already exists, add a suffix like -2025 or your project name.\nğŸ“‚ 3.2. Check and configure the Bucket Once created, you will see your bucket in the list.\nClick on the bucket name to open the details.\nGo to the Properties tab to check the overall configuration.\nGo to the Permissions tab to configure access (important if you want to save the frontend publicly).\nâš ï¸ If you plan to host a static website from S3, you need to allow public access. Make sure you understand the security risks of enabling public access.\nâœ… Done ğŸ‰ You have successfully created an S3 Bucket â€“ the central storage component in your project\u0026rsquo;s serverless architecture.\nIn the next steps, we will:\nUpload the built frontend to the bucket â€‹â€‹Connect CloudFront to distribute content globally Configure Lambda to write/read data if needed ğŸ“Œ Summary: S3 is a platform for storing static data and content for web applications. Make sure the bucket is in the same region as other services to avoid access errors and optimize costs.\n"
},
{
	"uri": "//localhost:1313/3-create-s3-bucket/",
	"title": "Create an S3 Bucket to Store Data",
	"tags": [],
	"description": "",
	"content": "In this section, we will create an Amazon S3 bucket to serve as a central storage for all data for the Machine Learning Pipeline project.\nS3 will act as a data lake â€“ a place to store input data (raw data), pre-processed data (processed data), model artifacts as well as prediction data (inference results). This is an important starting point for the pipeline to operate automatically and connect between AWS services.\nğŸ’¡ Hint: Always choose the same region for S3, Lambda and SageMaker to reduce latency and avoid access errors.\nContents 3.1. Create an S3 Bucket to store data 3.2. S3 Data Organization Structure 3.3. Upload sample data to S3 to test pipeline ğŸ“¦ Amazon S3 (Simple Storage Service) is an object storage service that makes it easy to manage big data, automatically scales, and seamlessly integrates with services like Lambda, SageMaker, and API Gateway.\n"
},
{
	"uri": "//localhost:1313/5-train-model-with-sagemaker/5.3-register-and-manage-model/",
	"title": "Register and Manage Models in SageMaker Model Registry",
	"tags": [],
	"description": "",
	"content": "In this section, we will register the trained model to the SageMaker Model Registry for version management, metadata tracking, and future deployment. This is an important step to make your ML pipeline repeatable, version-controlled, and deployable.\nğŸ¯ Objectives Register the trained model from S3 output to the SageMaker Model Registry.\nAttach metadata (version, data information, hyperparameters, metrics).\nManage model versions and approval status (Approved/Pending/Rejected).\nğŸ§  1. Overview of SageMaker Model Registry SageMaker Model Registry is a ML model lifecycle management service that allows you to:\nStore and manage model versions over time.\nAttach model information such as metrics, hyperparameters, training data.\nControl the model approval process before deployment.\nIntegrate directly with SageMaker Endpoint to deploy production models.\nğŸ“Š Architecture after adding Model Registry:\nğŸ“ 2. Access and create Model Package Group Go to AWS Management Console â†’ select Amazon SageMaker. In the left menu, select Model registry â†’ Model package groups. Click Create model package group. Fill in the information:\nName: ml-pipeline-model-group Description: â€œModel group for ML pipeline workshopâ€ Click Create model package group. ğŸ“¤ 3. Register the trained model Now we will create a Model Package from the saved model after training (model.tar.gz) in S3.\nIn Model registry â†’ select the newly created group â†’ Create model package. Configure as follows: Model package name: ml-pipeline-model-v1\nModel location (S3): s3://ml-pipeline-bucket/model/model.tar.gz\nInference image URI:\nIf using built-in XGBoost: select the available container from SageMaker.\nIf custom: enter your ECR container image.\nIAM Role: SageMakerExecutionRole (has access to S3 and SageMaker).\nApproval status: Pending manual approval (or Approved if ready to deploy).\nğŸ§ª 4. Attach metadata \u0026amp; model metrics (optional) You can attach important information to help the ML/DevOps team understand the model:\nTraining dataset version: v1.0 Algorithm: XGBoost Accuracy: 0.912 Hyperparameters: learning_rate, max_depth, n_estimators Created by: Lambda Preprocessing Pipeline ğŸ“Œ This is useful when you have multiple models and need to choose the best one to deploy.\nâœ… 5. Manage model versions Every time you create a new Model Package, it becomes a version in the group:\nVersion Model Name Accuracy Status Approval 1 ml-pipeline-model-v1 0.912 Completed Approved âœ… 2 ml-pipeline-model-v2 0.927 Completed Pending ğŸ• ğŸ“ˆ You can update the model status from Pending â†’ Approved once the review is complete.\nğŸ” 6. Check the registered model using AWS CLI (optional) If you want to automate, you can register the model using CLI:\naws sagemaker create-model-package \\ --model-package-group-name ml-pipeline-model-group \\ --model-package-description \u0026#34;ML pipeline v1 model\u0026#34; \\ --model-approval-status Approved \\ --inference-specification \u0026#39;{\u0026#34;Containers\u0026#34;: [{\u0026#34;Image\u0026#34;: \u0026#34;\u0026lt;IMAGE_URI\u0026gt;\u0026#34;, \u0026#34;ModelDataUrl\u0026#34;: \u0026#34;s3://ml-pipeline-bucket/model/model.tar.gz\u0026#34;}], \u0026#34;SupportedContentTypes\u0026#34;: [\u0026#34;text/csv\u0026#34;], \u0026#34;SupportedResponseMIMETypes\u0026#34;: [\u0026#34;text/csv\u0026#34;]}\u0026#39; ğŸ§¹ 7. Update and control model lifecycle When training a new model, simply create a new Model Package in the same Model Package Group.\nThis helps track model history and makes it easy to rollback if the new model fails.\nğŸ¯ Done You have successfully registered your trained model in the SageMaker Model Registry, managing version and approval status.\nThis is an important step to keep your ML pipeline organized and make it easy to deploy models into production.\n"
},
{
	"uri": "//localhost:1313/4-implement-lambda-preprocessing/",
	"title": "Implementing a Lambda Preprocessing Function",
	"tags": [],
	"description": "",
	"content": "In this section, we will build a Lambda Preprocessing Function to automatically process raw data uploaded to S3. This is an important initial step in the machine learning pipeline â€” ensuring the data is clean, standardized, and ready for training models on Amazon SageMaker.\nğŸ¯ Goals Build a Lambda function in Python to process CSV data from S3.\nAutomatically trigger Lambda when a new file is added to the raw/ directory.\nWrite the processed results to the processed/ directory in the same bucket.\nEnsure the data is ready for training a SageMaker model.\nğŸ§  Architecture Overview Here\u0026rsquo;s how Lambda Preprocessing Function works in the entire pipeline:\nğŸ“ S3 Bucket â”œâ”€â”€ raw/ â† raw CSV uploaded â”œâ”€â”€ processed/ â† CSV processed, ready to train Step 1: User or system uploads CSV file to raw/.\nStep 2: S3 activates Lambda via trigger.\nStep 3: Lambda reads file, cleans data (removes error lines, handles empty valuesâ€¦).\nStep 4: Lambda writes processed file to processed/.\nğŸ› ï¸ 4.1 â€“ Create Lambda Function Go to AWS Lambda Console.\nClick Create function.\nConfiguration:\nFunction name: preprocessData Runtime: Python 3.9 Role: Select the IAM role created in the previous step (with S3 permissions). ğŸ“œ 4.2 â€“ Write code to process data In the Code tab, replace the default content with the following code:\nimport boto3 import csv import io import os s3 = boto3.client(\u0026#39;s3\u0026#39;) def lambda_handler(event, context): # Láº¥y bucket vÃ  key cá»§a file má»›i upload bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] print(f\u0026#34;Processing file: s3://{bucket}/{key}\u0026#34;) # Äá»c file CSV tá»« S3 try: response = s3.get_object(Bucket=bucket, Key=key) raw_bytes = response[\u0026#39;Body\u0026#39;].read() try: content = raw_bytes.decode(\u0026#39;utf-8\u0026#39;) # Thá»­ decode UTF-8 except UnicodeDecodeError: content = raw_bytes.decode(\u0026#39;utf-16\u0026#39;) # Náº¿u lá»—i, thá»­ UTF-16 except Exception as e: print(f\u0026#34;Error reading file from S3: {e}\u0026#34;) raise e reader = csv.reader(io.StringIO(content)) processed_rows = [] for row in reader: # Bá» qua dÃ²ng rá»—ng hoáº·c lá»—i if all(row): processed_rows.append(row) # Viáº¿t file Ä‘Ã£ xá»­ lÃ½ vÃ o folder processed/ output_prefix = os.getenv(\u0026#39;OUTPUT_PREFIX\u0026#39;, \u0026#39;processed/\u0026#39;) output_key = os.path.join(output_prefix, os.path.basename(key)) output_content = io.StringIO() writer = csv.writer(output_content) writer.writerows(processed_rows) # Upload file Ä‘Ã£ xá»­ lÃ½ lÃªn S3 try: s3.put_object( Bucket=bucket, Key=output_key, Body=output_content.getvalue().encode(\u0026#39;utf-8\u0026#39;), ContentType=\u0026#39;text/csv\u0026#39; ) print(f\u0026#34;Processed file saved to s3://{bucket}/{output_key}\u0026#34;) except Exception as e: print(f\u0026#34;Error writing file to S3: {e}\u0026#34;) raise e return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: f\u0026#34;Processed file saved to {output_key}\u0026#34; } âš™ï¸ 4.3 â€“ Assign Environment Variables In the Configuration â†’ Environment variables tab, click Edit and add:\nKey: OUTPUT_PREFIX Value: processed/ Environment variables help you easily change the output path without editing the source code.\nğŸ“¦ 4.4 â€“ Package and Upload Lambda (local option) If developing on a local computer:\npip install -r requirements.txt -t . zip -r preprocessData.zip . Then go back to Lambda Console â†’ Code tab â†’ Upload from â†’ .zip file.\nCreate a requirements.txt file with the following content:\nboto3 ğŸ”” 4.5 â€“ Connect Lambda to S3 (Trigger) In Lambda Console, go to Configuration â†’ Triggers tab.\nClick Add trigger.\nSelect:\nTrigger type: S3 Bucket: your-raw-data-bucket Event type: PUT Prefix: raw/ ğŸ“¸ Trigger configuration example:\nMake sure the bucket and Lambda are in the same region. Otherwise, the trigger will not work.\nğŸ§ª 4.6 â€“ Test the Preprocessing Function Upload a sample CSV file to the raw/ folder:\naws s3 cp data.csv s3://ml-pipeline-bucket/raw/data.csv Lambda will automatically run and create the processed file in processed/. ğŸ“Š Example result:\nInput: s3://ml-pipeline-bucket/raw/data.csv Output: s3://ml-pipeline-bucket/processed/data.csv âœ… Done ğŸ‰ You have successfully deployed a Lambda Preprocessing Function â€“ the first step in a machine learning pipeline.\n"
},
{
	"uri": "//localhost:1313/3-create-s3-bucket/3.2-organize-data-structure/",
	"title": "Organizing Data Structures",
	"tags": [],
	"description": "",
	"content": "In this section, you will learn how to organize data structures (folders and files) inside the Amazon S3 bucket created in the previous step. Properly organizing data makes it easier for the frontend (React/Vite) to be distributed via CloudFront and for the backend to access content properly.\nğŸ“ Why is it important to organize data structures? An S3 bucket does not have the concept of real folders â€“ it organizes data by key prefix (string of file names). Properly organizing data will help:\nEasily manage and update frontend content. Fine-grain access permissions if there are multiple teams working on it later. Optimize cache when serving via CloudFront. ğŸ§± Recommended folder structure Create the following structure in the S3 bucket:\nmy-blog-frontend/ â”‚ â”œâ”€ index.html â”œâ”€ favicon.ico â”œâ”€ /assets/ â”‚ â”œâ”€ logo.png â”‚ â””â”€ styles.css â”œâ”€ /js/ â”‚ â””â”€ main.js â””â”€ /posts/ â””â”€ sample-post.json ğŸ“„ index.html â€“ root file of React/Vite application. ğŸ“ /assets/ â€“ contains images, CSS. ğŸ“ /js/ â€“ contains JavaScript files built from the frontend application. ğŸ“ /posts/ â€“ (optional) contains sample posts or static JSON content. ğŸª„ Add sample data to test the bucket Go to Amazon S3 Console. Select the bucket you created in step 3.1. Click Upload. Upload the index.html file and the folders as above. After uploading, you will see the folder structure in the bucket similar to the image: The folder name is not required but should follow the above structure for easy integration with CloudFront and CI/CD later. Make sure the index.html file is in the root of the bucket, as that is the entry point of the website. If you use React Router, enable Static website hosting in the â€œPropertiesâ€ section so that S3 can handle routing properly. âœ… Done You have successfully organized the data structure in the S3 bucket and added sample data to prepare for the frontend deployment step.\n"
},
{
	"uri": "//localhost:1313/5-train-model-with-sagemaker/5.4-validate-training-results/",
	"title": "Validate Model Training Results",
	"tags": [],
	"description": "",
	"content": "In this section, we will learn how to validate training results to ensure that the model has been successfully trained and is of good quality before deploying to production. This is a very important step in the ML pipeline process because it helps you evaluate the quality of the model, check the output and decide whether to deploy or need to re-tune the data/hyperparameters.\nğŸ¯ Objectives Check the status of the training job in SageMaker. Analyze the log and output of the training process. Evaluate the metrics (accuracy, F1, AUCâ€¦) of the model. Validate the output model file before putting it into the Model Registry or deploying. ğŸ“Š 1. Check Training Job Status Go to AWS Management Console â†’ Amazon SageMaker â†’ Training jobs to see the list of training jobs created in step 5.2 â€“ Create Training Job.\nThe Status column will display the status: âœ… Completed â€“ Completed successfully. âŒ Failed â€“ Training failed (check log). ğŸ• InProgress â€“ Training in progress. If the status is Failed, check the CloudWatch logs to determine the cause (e.g., S3 permission error, training script code error, or missing data).\nğŸ“ 2. Check the Model Output in S3 After successful training, SageMaker will save the model to S3 at the path you specified: s3://ml-pipeline-bucket/model/model.tar.gz\nmodel.tar.gz contains the serialized model (e.g., pickle, joblib, or framework format like TensorFlow SavedModel).\nYou can download this file to your computer and check the structure to ensure the content is valid.\nğŸ“œ 3. Analyze logs in CloudWatch Go to CloudWatch Logs â†’ Log groups â†’ find the log group corresponding to the training job.\nView the log output from the training script - you will see information like:\n[INFO] Training accuracy: 0.912 [INFO] Validation accuracy: 0.905 [INFO] Model saved to /opt/ml/model/ ğŸ“Œ If you write metrics to stdout or save to /opt/ml/output/metrics.json, SageMaker will automatically collect them to display in the interface.\nğŸ“ˆ 4. Evaluate training metrics Metrics are the main measure to decide whether the model can be put into production or not.\nSome common metrics:\nMetric Significance Accuracy Ratio of correct predictions to total samples. Precision Accuracy when the model predicts Positive. Recall Ability to detect all Positive cases. F1-Score Harmonic average between Precision and Recall. AUC-ROC Ability to discriminate between classes. You should set a minimum threshold for these metrics (e.g. Accuracy \u0026gt; 0.90) to automatically approve or reject the model before registration.\nğŸ”¬ 5. Validate the model\u0026rsquo;s correctness Check if the model can be loaded and inferenced locally: import joblib model = joblib.load(\u0026#34;model.tar.gz\u0026#34;) print(model.predict([[5.1, 3.5, 1.4, 0.2]])) If using TensorFlow/PyTorch, check with the load_model() or torch.load() function.\nIf the model cannot be loaded, the training script may not have saved the model in the correct format.\nâœ… 6. Evaluate and Decide Based on the validation results, you can make the following decisions:\nâœ… If the model meets the requirements â†’ Continue registering to the Model Registry or deploying.\nâš ï¸ If the model is not good enough â†’ Adjust data, hyperparameters or model â†’ retrain.\nğŸ“Š Example of evaluation results:\nMetric Value Required threshold Conclusion Accuracy 0.912 \u0026gt;0.90 âœ… Passed Precision 0.908 \u0026gt;0.85 âœ… Passed Recall 0.901 \u0026gt;0.85 âœ… Passed F1-Score 0.905 \u0026gt;0.85 âœ… Passed ğŸ‰ Done You have successfully validated the model training results in SageMaker This is an important step to ensure quality before the model is registered and deployed. "
},
{
	"uri": "//localhost:1313/5-train-model-with-sagemaker/",
	"title": "Train and Register Models with Amazon SageMaker",
	"tags": [],
	"description": "",
	"content": "In this section, we will train a machine learning model with Amazon SageMaker using preprocessed data from Lambda and stored on S3. This is an important step to create a model that is ready to serve inference via API later.\nğŸ¯ Objectives Learn how to create a SageMaker Training Job from processed data.\nConfigure training parameters and choose algorithms.\nRegister the model (Model Registry) for later deployment.\nManage model versions and track training progress.\nğŸ“š Contents 5.1 Prepare input data for SageMaker 5.2 Create SageMaker Training Job 5.3 Register model and manage version 5.4 Check training results ğŸ§  Architecture overview Lambda creates processed data and saves it to the processed/ folder on S3.\nSageMaker reads data from S3, trains the model using the algorithm of your choice.\nThe resulting model is stored in model/ and can be registered in the Model Registry.\nThe registered model will be used to deploy inference in the next step.\nğŸ“¦ Prerequisites Completed Lambda Preprocessing Function (Chapter 4).\nHas processed data in the processed/ folder of the S3 bucket.\nSageMaker IAM Role with access to S3, CloudWatch, and SageMaker.\nâœ… After completing this chapter, you will have a trained and registered ML model\n"
},
{
	"uri": "//localhost:1313/3-create-s3-bucket/3.3-upload-sample-data/",
	"title": "Upload sample data",
	"tags": [],
	"description": "",
	"content": "In this section, we will upload sample data (frontend build) to Amazon S3 bucket to test the accessibility of the static website before connecting to API Gateway and Lambda.\nğŸ“¦ Why upload sample data? Uploading sample data to S3 helps you:\nâœ… Verify that the bucket is working correctly and the website can be distributed. âœ… Test the accessibility of the React/Vite frontend via the browser. âœ… Ensure the data structure is correct before integrating with the backend. Step 1 â€“ Build the frontend application If you use React + Vite, in the frontend folder run:\nnpm install npm run build This command will create a dist/ (or build/) folder containing static files ready to upload to S3.\nThe folder structure after build can be as follows: dist/ â”‚ â”œâ”€ index.html â”œâ”€ favicon.ico â”œâ”€ assets/ â”‚ â”œâ”€ main.js â”‚ â””â”€ style.css â””â”€ logo.png Step 2 - Upload frontend build to S3 Access Amazon S3 Console Select the bucket you created in step 3.1 Create S3 Bucket â€‹â€‹Click Upload Select all contents in dist/ (or build/) folder Click Upload to complete the upload process Example to illustrate the upload process: Step 3 - Enable Static Website Hosting (if not enabled) In the Properties tab of the bucket â€‹â€‹Scroll down to the Static website section hosting Select Enable Specify: Index document: index.html Error document: index.html (if using React Router) The index.html file must be located at the root of the bucket, not in a subfolder. If the website has routing (SPA), set Error document = index.html.\nStep 4 â€“ Test website access Copy the Static website endpoint URL from the bucket configuration. Paste the URL into the browser to check the result. Example: http://my-blog-frontend.s3-website-ap-southeast-1.amazonaws.com Result when accessing successfully: Important note Check the permissions of the bucket again if you cannot access the website. Make sure the index.html file has been uploaded to the correct location. If you want to use CloudFront in the future, you don\u0026rsquo;t need to enable \u0026ldquo;Public access\u0026rdquo; for the bucket â€“ CloudFront will access it for you.\nâœ… Done You have successfully uploaded the sample frontend build to the S3 bucket and verified the accessibility of the static website.\n"
},
{
	"uri": "//localhost:1313/6-deploy-sagemaker-endpoint/",
	"title": "Deploy SageMaker Endpoint for Inference",
	"tags": [],
	"description": "",
	"content": "\nIn this section, we will deploy the trained model from step 5 to Amazon SageMaker Endpoint, allowing inference calls via API or Lambda function. This is an important step to turn your ML model into a service that can be used in the real world.\nğŸ¯ Objectives Create an endpoint from the model trained and registered in the previous step. Test the endpoint with sample data. Prepare the endpoint for integration with Lambda in the next step. ğŸ§  6.1 â€“ Create a SageMaker Model from an Artifact After training and registering the model (step 5), we will create a SageMaker Model based on that output.\nGo to SageMaker Console â†’ Inference â†’ Models Click Create model Configure: Model name: ml-blog-model Execution role: Select the IAM role created in the previous step (SageMakerExecutionRole) Container definition: Image: 382416733822.dkr.ecr.ap-southeast-1.amazonaws.com/xgboost:latest (or the image you used when training) Model artifact location: s3://ml-pipeline-bucket/model/xgboost-model.tar.gz Click Create model to complete. ğŸ“Œ Note: The container image and artifact path must match the previously created train job.\nâš™ï¸ 6.2 â€“ Create Endpoint Configuration Navigate to Inference â†’ Endpoint configurations Select Create endpoint configuration Configuration: Name: ml-blog-endpoint-config Model name: ml-blog-model Instance type: ml.m5.large (or ml.t2.medium if you want to save costs) Initial instance count: 1 Click Create endpoint configuration ğŸŒ 6.3 â€“ Deploy SageMaker Endpoint Navigate to Inference â†’ Endpoints Select Create endpoint Enter: Endpoint name: ml-blog-endpoint Endpoint configuration: Select ml-blog-endpoint-config Click Create endpoint. The creation process will take a few minutes â³. ğŸ“¸ Example deployment interface:\nMake sure the IAM Role has access to S3 and SageMaker (AmazonSageMakerFullAccess, AmazonS3ReadOnlyAccess). The endpoint must be in InService state before using inference. ğŸ§ª 6.4 â€“ Testing Endpoint with boto3 (Python) Once the endpoint is in InService state, check the inference with the following Python code:\nimport boto3 import json runtime = boto3.client(\u0026#39;sagemaker-runtime\u0026#39;) payload = { \u0026#34;features\u0026#34;: [0.56, 0.32, 0.78, 0.12] # example input data } response = runtime.invoke_endpoint( EndpointName=\u0026#39;ml-blog-endpoint\u0026#39;, ContentType=\u0026#39;application/json\u0026#39;, Body=json.dumps(payload) ) result = json.loads(response[\u0026#39;Body\u0026#39;].read().decode()) print(\u0026#34;ğŸ“Š Predicted result:\u0026#34;, result) âœ… The result will return the predicted value (e.g., 1 or 0 for a classification model). .\nğŸ“Š 6.5 â€“ Endpoint Monitoring with CloudWatch Go to CloudWatch â†’ Logs to view the inference log. Monitor metrics such as:\nInvocations Invocation4XXErrors ModelLatency This helps evaluate model performance in production environments. ğŸ“Œ You can enable Auto Scaling for the endpoint by using Application Auto Scaling to automatically scale up/down the number of instances based on inference traffic.\nâœ… Done You have successfully deployed a SageMaker Endpoint from the trained model.\n"
},
{
	"uri": "//localhost:1313/7-build-lambda-inference-and-api/",
	"title": "Build Lambda Function and REST API for Inference",
	"tags": [],
	"description": "",
	"content": "\nIn this section, we will implement a Lambda function to call the model deployed on SageMaker Endpoint (step 6) and create a REST API Gateway so that the client can send inference requests. This is the important link to help turn the ML model into a complete prediction service.\nğŸ¯ Goals Create a Lambda function that calls SageMaker Endpoint to handle inference.\nConnect Lambda to API Gateway to create REST API.\nCheck inference from Postman or browser.\nğŸ§  7.1 â€“ Create Lambda Function to Call SageMaker Endpoint Go to AWS Management Console â†’ Lambda â†’ Create function Configure: Function name: invoke-ml-endpoint Runtime: Python 3.9 Execution role: Select the role that has permission to call SageMaker (or create a new role with permissions AmazonSageMakerFullAccess and AWSLambdaBasicExecutionRole) Click Create function âœï¸ 7.2 â€“ Write Lambda code to call Endpoint Replace the default content in the Code tab with the following code:\nimport json import boto3 import os runtime = boto3.client(\u0026#39;sagemaker-runtime\u0026#39;) ENDPOINT_NAME = os.environ.get(\u0026#39;ENDPOINT_NAME\u0026#39;, \u0026#39;ml-blog-endpoint\u0026#39;) def lambda_handler(event, context): try: body = json.loads(event[\u0026#39;body\u0026#39;]) features = body.get(\u0026#39;features\u0026#39;) if features is None: return { \u0026#34;statusCode\u0026#34;: 400, \u0026#34;body\u0026#34;: json.dumps({\u0026#34;error\u0026#34;: \u0026#34;Missing \u0026#39;features\u0026#39; in request body\u0026#34;}) } response = runtime.invoke_endpoint( EndpointName=ENDPOINT_NAME, ContentType=\u0026#39;application/json\u0026#39;, Body=json.dumps({\u0026#34;features\u0026#34;: features}) ) result = json.loads(response[\u0026#39;Body\u0026#39;].read().decode()) return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;headers\u0026#34;: {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}, \u0026#34;body\u0026#34;: json.dumps({\u0026#34;prediction\u0026#34;: result}) } except Exception as e: return { \u0026#34;statusCode\u0026#34;: 500, \u0026#34;body\u0026#34;: json.dumps({\u0026#34;error\u0026#34;: str(e)}) } ğŸ“Œ Explanation:\nENDPOINT_NAME: endpoint name deployed in step 6. Lambda receives JSON data from client (features), calls SageMaker endpoint, and returns prediction results. In Configuration â†’ Environment variables, add variables: Key: ENDPOINT_NAME Value: ml-blog-endpoint Click Deploy to save the function. ğŸŒ 7.3 â€“ Create REST API Gateway connecting Lambda Go to API Gateway â†’ Create API Select REST API â†’ Build API name: InferenceAPI Endpoint Type: Regional Create resource /predict: In Resources, select Actions â†’ Create Resource Resource name: predict Resource path: /predict Enable API Gateway CORS â†’ Create Resource Add POST method: Select /predict â†’ Actions â†’ Create Method â†’ â€‹â€‹POST Integration type: Lambda Function Lambda Function: invoke-ml-endpoint Click Save and confirm permissions. ğŸ”„ 7.4 â€“ Enable CORS and Deploy API In Resources, select Actions â†’ Enable CORS Keep the default configuration and click Enable CORS and replace existing CORS headers Deploy API: Actions â†’ Deploy API Deployment stage: [New Stage] â†’ name prod Click Deploy ğŸ“Œ Save the Invoke URL for example:\nhttps://abc123xyz.execute-api.ap-southeast-1.amazonaws.com/prod/predict ğŸ§ª 7.5 â€“ Test API Inference You can use Postman or curl command to test:\ncurl -X POST \\ https://abc123xyz.execute-api.ap-southeast-1.amazonaws.com/prod/predict \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;features\u0026#34;: [0.45, 0.12, 0.88, 0.33] }\u0026#39; âœ… Sample response: { \u0026#34;prediction\u0026#34;: 1 } ğŸ“Š 7.6 â€“ Logging and monitoring Check Lambda logs in CloudWatch Logs â†’ help debug if errors occur.\nMonitor metrics like Invocations, 4XXError, Latency to ensure API stability.\nğŸ’¡ You can add API authentication using API Keys, Cognito User Pools, or IAM Auth if deploying in production.\nâœ… Done ğŸ‰ You have successfully built a Lambda function to call SageMaker Endpoint, created REST API Gateway, and successfully tested inference.\n"
},
{
	"uri": "//localhost:1313/8-integrate-dynamodb-and-cloudwatch/8.1-create-dynamodb-table/",
	"title": "Creating a DynamoDB Table to Store Metadata Inference",
	"tags": [],
	"description": "",
	"content": "Amazon DynamoDB is a fully managed, high-performance, and auto-scalable NoSQL database service. In this project, DynamoDB will be used to store metadata of inference requests from the model, including input, output, processing time, and model information.\nThis makes it easy for us to analyze, monitor, and retrieve inference history when needed.\nDynamoDB in Machine Learning Pipeline Project In this section, we will:\nCreate a DynamoDB table to store metadata for each inference.\nDefine the Primary key and important fields.\nConfigure access permissions so Lambda can write data to the table.\nCheck the table and verify that the data was written successfully.\nğŸ¯ Benefits of using DynamoDB: Serverless \u0026amp; Auto-Scaling: No need to manage servers or data partitions.\nHigh Performance: Meet millions of requests per second.\nEasy Integration: Work directly with Lambda and other AWS services.\nInference History: Keep track of input, output, and inference time details.\nğŸ› ï¸ Create a DynamoDB table Go to AWS Management Console In the search bar, type DynamoDB and select the service. Create new table Select Create table.\nTable name: Enter the table name, e.g., InferenceMetadata.\nPartition key: Enter requestId (of type String).\nNo need to add a Sort key (optional).\nKeep the remaining settings default.\nClick Create table.\nSuggested data structure Each record in the table can contain the following fields:\nField Data Type Description requestId String Unique ID for each inference request timestamp String Time taken to perform inference modelName String Model name used inputData String Input data prediction String Model result returned latencyMs Number Latency (ms) of inference process Grant access to Lambda Open IAM Console, select Lambda inference role (e.g. lambda-inference-role).\nAdd DynamoDB access by attaching the following policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:DescribeTable\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:table/InferenceMetadata\u0026#34; } ] } Ensure the Partition key is unique to avoid overwriting data. Record the table name for use in the Lambda configuration step (8.2). The Lambda role must have PutItem permission to write data to the table.\nCheck the table After Lambda writes the inference data, go to the Explore table items tab of the InferenceMetadata table.\nYou will see the records containing full information for each inference. âœ… Done You have created a DynamoDB table to store the inference metadata.\nThis is the foundation for Lambda to write data after each prediction, serving the analysis and monitoring goals in the next step.\n"
},
{
	"uri": "//localhost:1313/8-integrate-dynamodb-and-cloudwatch/",
	"title": "Integrating DynamoDB and CloudWatch",
	"tags": [],
	"description": "",
	"content": "DynamoDB \u0026amp; CloudWatch in ML Pipeline In the final step of the pipeline, we will integrate Amazon DynamoDB to store metadata of inference requests and model information, and use Amazon CloudWatch to monitor logs, measure performance, and generate alerts when problems occur.\nThis is an important step to ensure the pipeline can operate sustainably in a real environment.\nğŸ—ƒï¸ DynamoDB â€“ Storing metadata inference Amazon DynamoDB is a high-performance, auto-scalable, serverless NoSQL database. In this project, DynamoDB will be used to:\nStore inference results from Lambda (input, output, time). Save model information such as version, endpoint name. Serve performance monitoring and analysis later. ğŸ“Š CloudWatch â€“ Pipeline monitoring and analysis Amazon CloudWatch is a central monitoring service on AWS. It will help you:\nMonitor logs from Lambda and SageMaker Endpoint. Create metric filters to analyze the number of inferences, errors, and delays. Configure alarm when the pipeline has problems. ğŸ“š Contents 8.1 Create a DynamoDB table to store metadata inference 8.2 Update Lambda to write data to DynamoDB 8.3 Monitor and alert with CloudWatch ğŸ“Œ Summary\nâœ… You will learn how to create and manage a DynamoDB table to store inference results. âœ… Lambda will be extended to write data each time a prediction is made. âœ… CloudWatch will help you monitor logs, analyze performance, and create alerts. ğŸ¯ Outcomes after this chapter:\nA complete ML pipeline capable of historical inference, automatic monitoring, and early warning when problems occur.\nReady to operate in production environments with easy scalability and maintenance.\n"
},
{
	"uri": "//localhost:1313/9-clean-up-resources/",
	"title": "Clean Up Resources",
	"tags": [],
	"description": "",
	"content": "After completing the entire \u0026ldquo;Building an End-to-End Machine Learning Pipeline on AWS\u0026rdquo; workshop, the final step is to clean up all AWS resources that you created during the deployment.\nThis is extremely important to avoid incurring costs outside of the Free Tier, ensuring security and keeping your AWS account clean for future projects.\nğŸ¯ Goal of this step ğŸ’¸ Cost Savings: Prevent costs from services that are no longer in use. ğŸ” Security: Remove unnecessary IAM, API and resource permissions to reduce security risks. ğŸ§° Clean and easy to manage: Keep your AWS account clean and ready for new workshops or projects. ğŸ—‘ï¸ Resources to delete Here is a list of resources that have been used throughout the project that you need to delete:\nAmazon CloudFront â€“ CDN for delivering applications and content. Amazon S3 â€“ Stores data and models. Amazon API Gateway â€“ REST API connecting Lambda and endpoint inference. AWS Lambda â€“ Functions for data processing, preprocessing, and inference. Amazon DynamoDB â€“ Table for storing metadata and inference results. AWS IAM â€“ Roles and permissions for Lambda and SageMaker. ğŸ§¼ Detailed Cleanup Guide 1. ğŸ§­ Delete a CloudFront Distribution Go to CloudFront from the AWS Management Console. Select the distribution you created (e.g., d1234567890abcdef.cloudfront.net). Click Disable and wait for the status to change to Disabled. Click Delete to completely delete the distribution. 2. ğŸ“¦ Delete an Amazon S3 bucket Go to S3 from the AWS console. Select the bucket you created (e.g., ml-workshop-data-\u0026lt;account-id\u0026gt;). Click Empty, enter permanently delete to confirm, then select Empty. When the bucket is empty, click Delete bucket, enter the bucket name to confirm the deletion. 3. ğŸŒ Delete API Gateway Go to API Gateway. Select the API you deployed (e.g., InferenceAPI). Click Actions â†’ Delete, enter the API name to confirm and complete the deletion. 4. ğŸ§  Delete AWS Lambda functions Go to Lambda from the console. Delete all the functions you created, e.g., preprocessing-function inference-function Click Actions â†’ Delete, confirm the deletion of each function. 5. ğŸ“Š Delete a DynamoDB table Go to DynamoDB â†’ Tables. Select the table you created (e.g., InferenceMetadata). Click Actions â†’ Delete table, enter the table name to confirm. 6. ğŸ” Delete IAM resources Go to IAM in the console. In Policies, select the created policy (e.g., lambda-inference-policy) â†’ Delete. In Roles, select the related role (e.g., lambda-inference-role) â†’ Delete. âš ï¸ Important Notes:\nMake sure your S3 bucket is empty before deleting. Double-check your resources before deleting to avoid accidentally deleting resources from other projects. If you encounter errors when deleting (e.g., a resource is still referenced), check any dependencies such as IAM permissions, API Gateway endpoints, or CloudFront distributions. âœ… Cleanup Results All project resources have been deleted. Your AWS account no longer has any resources that incur charges. You can start new workshops or projects without conflicting with old resources. "
},
{
	"uri": "//localhost:1313/8-integrate-dynamodb-and-cloudwatch/8.2-update-lambda-to-write-dynamodb/",
	"title": "Update Lambda to Write Data to DynamoDB",
	"tags": [],
	"description": "",
	"content": "\nIn this section, we will:\nUpdate the Lambda inference function to write metadata each time the model is called into DynamoDB.\nVerify that the data is written successfully.\nVerify that the inference process is fully tracked.\nğŸ¯ Goals of this step: Storing inference history: Record input data, results, processing time, and model information.\nEasy Analysis \u0026amp; Debugging: Stored data helps track model behavior over time.\nIntegrated with Monitoring: Prepare for connecting to CloudWatch to monitor the pipeline.\nğŸ› ï¸ Update Lambda to write data to DynamoDB Open Lambda Function Inference Go to AWS Lambda in the Management Console.\nSelect the Lambda function you created to call SageMaker Endpoint (e.g., ml-inference-lambda).\nUpdate Lambda source code Replace the contents of the lambda_function.py file with the code below to write metadata to the InferenceMetadata table after each inference:\nimport json import boto3 import os import time import uuid from datetime import datetime sagemaker = boto3.client(\u0026#39;sagemaker-runtime\u0026#39;) dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) ENDPOINT_NAME = os.environ[\u0026#39;ENDPOINT_NAME\u0026#39;] TABLE_NAME = os.environ[\u0026#39;DYNAMODB_TABLE\u0026#39;] def lambda_handler(event, context): # Parse input from API Gateway body = json.loads(event[\u0026#39;body\u0026#39;]) input_data = body[\u0026#39;input\u0026#39;] # Call the SageMaker endpoint start_time = time.time() response = sagemaker.invoke_endpoint( EndpointName=ENDPOINT_NAME, ContentType=\u0026#39;application/json\u0026#39;, Body=json.dumps(input_data) ) prediction = json.loads(response[\u0026#39;Body\u0026#39;].read()) latency_ms = int((time.time() - start_time) * 1000) # Write metadata to DynamoDB request_id = str(uuid.uuid4()) timestamp = datetime.utcnow().isoformat() dynamodb.put_item( TableName=TABLE_NAME, Item={ \u0026#39;requestId\u0026#39;: {\u0026#39;S\u0026#39;: request_id}, \u0026#39;timestamp\u0026#39;: {\u0026#39;S\u0026#39;: timestamp}, \u0026#39;modelName\u0026#39;: {\u0026#39;S\u0026#39;: ENDPOINT_NAME}, \u0026#39;inputData\u0026#39;: {\u0026#39;S\u0026#39;: json.dumps(input_data)}, \u0026#39;prediction\u0026#39;: {\u0026#39;S\u0026#39;: json.dumps(prediction)}, \u0026#39;latencyMs\u0026#39;: {\u0026#39;N\u0026#39;: str(latency_ms)} } ) # Returns inference results return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;requestId\u0026#39;: request_id, \u0026#39;prediction\u0026#39;: prediction, \u0026#39;latencyMs\u0026#39;: latency_ms }) } âœ… Including: ENDPOINT_NAME: deployed SageMaker Endpoint name.\nTABLE_NAME: DynamoDB table name created in 8.1.\nlatencyMs: inference time, in milliseconds.\nAdd environment variables to Lambda In Lambda configuration page â†’ Configuration â†’ Environment variables â†’ Edit.\nAdd variables:\nENDPOINT_NAME = SageMaker endpoint name.\nDYNAMODB_TABLE = InferenceMetadata.\n4. Check IAM access\nMake sure the Lambda role has permission to write data to DynamoDB as follows: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:DescribeTable\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:table/InferenceMetadata\u0026#34; } ] } Deploy and test Lambda Click Deploy to save changes. Use Test in Lambda Console or send a POST request to API Gateway as follows: curl -X POST \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;input\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;Hello AI!\u0026#34;}}\u0026#39; \\ https://\u0026lt;api-id\u0026gt;.execute-api.\u0026lt;region\u0026gt;.amazonaws.com/prod/inference Check the response contains requestId, prediction, and latencyMs. Verify data in DynamoDB Open the InferenceMetadata table in DynamoDB Console. Select Explore table items to view the data being logged. You should see the following logs: requestId timestamp modelName inputData prediction latencyMs 6c1a4\u0026hellip; 2025-10-01T10:20:30Z ml-endpoint {\u0026ldquo;text\u0026rdquo;: \u0026ldquo;Hello AI!\u0026rdquo;} {\u0026ldquo;result\u0026rdquo;: \u0026ldquo;positive\u0026rdquo;} 134 If data is not written, check Lambda\u0026rsquo;s IAM permissions. Verify the DynamoDB table name and environment variable are correct. View the detailed log in CloudWatch Logs to debug the error.\nâœ… Done You have updated Lambda to automatically write metadata each time you inference to DynamoDB. This is an important step to connect the inference pipeline to the storage and monitoring system. "
},
{
	"uri": "//localhost:1313/10-conclusion/",
	"title": "Conclusion and Key Points",
	"tags": [],
	"description": "",
	"content": "ğŸ“Œ Workshop Summary Congratulations ğŸ‰ â€“ you have completed the entire workshop \u0026ldquo;Building an End-to-End Machine Learning Pipeline on AWS\u0026rdquo;!\nThrough the previous 9 chapters, you have built an end-to-end automated â€“ scalable â€“ real-world Machine Learning system, including:\nData Storage (S3) â€“ storing input data and model output.\nLambda Functions â€“ preprocessing and inference without a server.\nAPI Gateway â€“ providing a RESTful API to connect models to external applications.\nSageMaker â€“ training, deploying and managing ML models at scale.\nDynamoDB â€“ stores metadata, inference results, and log models.\nCloudWatch â€“ monitors, logs, and optimizes system performance.\nCloudFront â€“ accelerates content delivery and secures HTTPS for applications.\nğŸš€ Real Values â€‹â€‹and Benefits This workshop is not just a technical lesson â€“ it is a complete model for real AI/ML projects. Understanding and implementing such a pipeline will help you:\nğŸ‘¨â€ğŸ’» For Engineers \u0026amp; Developers: Build a real-world ML Pipeline that tech companies are using. Automate the entire process from data collection â†’ training â†’ deployment â†’ inference. No need to manage servers (serverless) â€“ cost-effective and scalable. ğŸ§ª For Students Master modern ML architectures on the cloud â€“ a highly sought-after skill in the job market.\nDeep understanding of how AWS services work together in a complete AI system.\nğŸ§  Key knowledge to remember During the practice, you have approached many services and concepts. Here are the most important knowledge you need to master:\nTopics Core content Roles in the system Amazon S3 Storing training data, models and results Data platform AWS Lambda Running code without server (preprocessing \u0026amp; inference) Data processing and prediction Amazon SageMaker Training and deploying ML models The heart of the pipeline API Gateway Create a RESTful API that connects your application to your model Communicate with the outside world DynamoDB Store metadata, results, and model information Manage unstructured data CloudWatch Monitor logs, performance, and alerts System monitoring and oversight IAM Grant secure access between services Security and access control CloudFront Accelerate content delivery via CDN Application performance \u0026amp; security ğŸŒ Extension and Practical Applications This workshop can be the foundation for many real-world AI/ML applications such as:\nğŸ” Image/Text Classification â€“ you just need to change the training model in SageMaker.\nğŸ§  Time Series Prediction â€“ collect IoT data into S3, train, and deploy the predictive model.\nğŸ“Š Recommender System â€“ store user data, train models, and serve them via API Gateway.\nğŸ“± AI Backend for Mobile/Web Apps â€“ inference via Lambda and API Gateway at scale.\nğŸ› ï¸ What to learn next? To further advance your skills after this workshop, you can learn more:\nğŸ§¬ CI/CD for ML (MLOps) â€“ automate model training, testing, and deployment with CodePipeline or Step Functions.\nğŸ›¡ï¸ AWS WAF \u0026amp; Shield â€“ enhance API security and inference applications.\nğŸ“ˆ Advanced Monitoring â€“ use CloudWatch Dashboard or Grafana for detailed model monitoring.\nğŸ“¦ Containerization â€“ package models in Docker and deploy them using SageMaker or ECS/EKS.\nğŸ† Final Conclusion By completing this workshop, you will not only learn how to connect AWS services together, but also understand the entire lifecycle of a Machine Learning model in production â€“ from data to inference.\nğŸŒŸ This is the foundation of skills that modern ML engineers, Data Engineers, and Cloud Developers need to build AI systems that can be deployed in the real world.\n"
},
{
	"uri": "//localhost:1313/8-integrate-dynamodb-and-cloudwatch/8.3-monitor-with-cloudwatch/",
	"title": "Monitoring with Amazon CloudWatch",
	"tags": [],
	"description": "",
	"content": "\nAmazon CloudWatch is a system monitoring and observation service on AWS. In this project, CloudWatch helps monitor the entire inference pipeline â€” from Lambda, SageMaker Endpoint, to DynamoDB â€” to ensure performance, detect errors early, and optimize costs.\nğŸ¯ Goals of this section Collect and monitor logs from Lambda and SageMaker. Create custom metrics to track inference count, latency, and errors. Set up alarms when model or API performance is problematic. 1. Check logs from Lambda and SageMaker ğŸ“œ Log Lambda Go to Amazon CloudWatch â†’ Logs â†’ Log groups. Find the log group corresponding to Lambda (e.g. /aws/lambda/ml-inference-lambda). Check the details of each inference call, including: Time to call the endpoint. Input/output data. Inference time (latencyMs). Errors (if any). ğŸ“œ Log SageMaker Endpoint Go to Amazon CloudWatch â†’ Logs â†’ Log groups. Find the log group prefixed with /aws/sagemaker/Endpoints/ and select the corresponding endpoint. View the log for information: How many times the model was called. The response time of the model container. Errors during inference processing. Combine Lambda and SageMaker logs for faster error diagnosis when inference fails.\n2. Create Custom Metrics from Lambda For more detailed monitoring (e.g., inferences per minute, average latency), you can send Custom Metrics from Lambda to CloudWatch.\nUpdate the Lambda function as follows:\nimport boto3 import time import os cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) def publish_metrics(latency_ms, success=True): cloudwatch.put_metric_data( Namespace=\u0026#39;InferencePipeline\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;LatencyMs\u0026#39;, \u0026#39;Value\u0026#39;: latency_ms, \u0026#39;Unit\u0026#39;: \u0026#39;Milliseconds\u0026#39; }, { \u0026#39;MetricName\u0026#39;: \u0026#39;SuccessCount\u0026#39; if success else \u0026#39;ErrorCount\u0026#39;, \u0026#39;Value\u0026#39;: 1, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39; } ] ) # Call this function after each successful inference publish_metrics(latency_ms, success=True) Namespace: metric group name (InferencePipeline). LatencyMs: inference processing time. SuccessCount/ErrorCount: number of successful or error calls. 3. Create a pipeline monitoring dashboard Go to CloudWatch â†’ Dashboards â†’ Create dashboard. Name: Inference-Monitoring-Dashboard. Add widgets: ğŸ“Š Metric graph: LatencyMs chart over time. ğŸ“ˆ Number: total number of successful inferences (SuccessCount). âŒ Number: total number of error inferences (ErrorCount). Dashboard helps you monitor performance in real time, supporting model and resource optimization.\n4. Create an automatic alarm To receive alerts when the system has problems:\nGo to CloudWatch â†’ Alarms â†’ Create alarm. Select metric: LatencyMs \u0026gt; 2000 ms (2 seconds). Or ErrorCount \u0026gt; 0. Configure action: Send notification via Amazon SNS (email, SMS). Name: High-Latency-Alarm or Inference-Error-Alarm. 5. Check the entire inference flow Send a few requests to the inference API. Check: ğŸ“œ Lambda and SageMaker logs are fully displayed. ğŸ“Š Dashboard shows the number of inferences and latency. ğŸš¨ Alarms are triggered if the threshold is exceeded. If you don\u0026rsquo;t see metrics, check Lambda\u0026rsquo;s IAM permissions (cloudwatch:PutMetricData). Make sure Lambda sends metrics after each inference. Check the time zone when reading dashboard data.\nâœ… Done You have integrated and monitored your entire inference pipeline with Amazon CloudWatch. Your system can now log, measure performance, detect errors early, and send automatic alerts. "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]